---
title: "Hands On Machine Learning with R"
subtitle: "Fundamentals"
category: Data Splitting
comments: 
  utterances: 
    repo: ainsu23/ainsuasty_quarto
format: 
  html:
    css: css/blogs.css
---

```{r}
#| eval: true
#| include: false 
library(dplyr)
```

Getting very good understanding and skilled applying machine learning it is 
very important, for this reason, I started learning with the 
[book hands on machine learning with tidymodels](https://bradleyboehmke.github.io/HOML/)

This post contains notes and solutions to the exercises for each chapter of the 
book.

# Modelling process
## 1. Data Splitting `{rsample}`
Machine learning models requires data in order to teach the model. This data needs
to be separated in two. _Data used from package_: `{modeldata}`

::: {.panel-tabset}
### simple sample

```{r}
set.seed(123)  # for reproducibility
split <- rsample::initial_split(modeldata::attrition, prop = 0.7)
train <- rsample::training(split)
test <- rsample::testing(split)
rbind(
  table(train$Attrition) %>% prop.table(),
  table(test$Attrition) %>% prop.table()
) %>% as_tibble()
```

### Stratified sample

In case the variable response has imbalance, the split process should use stratify,
this helps to keep distribution of the response variable in the splitted data.

```{r}
set.seed(123)  # for reproducibility
split <- rsample::initial_split(modeldata::attrition, prop = 0.7,strata = "Attrition")
train <- rsample::training(split)
test <- rsample::testing(split)

rbind(
  table(train$Attrition) %>% prop.table(),
  table(test$Attrition) %>% prop.table()
) %>% as_tibble()

```

### Down-Sampling
"Down-sampling balances the dataset by reducing the size of the abundant class(es) 
to match the frequencies in the least prevalent class. This method is used when 
the quantity of data is sufficient. By keeping all samples in the rare class and 
randomly selecting an equal number of samples in the abundant class."

### Up-Sampling
"On the contrary, up-sampling is used when the quantity of data is insufficient. 
It tries to balance the dataset by increasing the size of rarer samples. Rather 
than getting rid of abundant samples, new rare samples are generated by using 
repetition or bootstrapping"
:::

### Train Data

**Train Data** "used to develop feature sets, train our 
algorithms, tune hyperparameters, compare models, and all of the other activities 
required to choose a final model (e.g., the model we want to put into production)."

```{mermaid}
flowchart LR
  id1[(DataBase)]  --> A((Train))
  subgraph Training
    direction TB
    subgraph Resampling
    B[resample 1]
    C[resample 2]
    D[resample 3]
    end
    subgraph Model_1
    E[Develop] --> F[Evaluate]
    G[Develop] --> H[Evaluate]
    I[Develop] --> J[Evaluate]
    end
    subgraph Model_2
    K[Develop] --> L[Evaluate]
    M[Develop] --> N[Evaluate]
    O[Develop] --> P[Evaluate]
    end
    subgraph Model_n
    Q[Develop] --> R[Evaluate]
    S[Develop] --> T[Evaluate]
    U[Develop] --> V[Evaluate]
    end
  end 
  A -- Data into samples --> Resampling
  Resampling -- Create --> Model_1
  Model_1 -- Tune --> Model_2
  Model_2 -- Tune --> Model_n
  
```

Once the best model is selected it is time to test the model with the test data.
Training (60% - 80%) and Testing (40% - 20%). It's importante to not pass this limits
because you can fall in a overfitting.


### Test data
**Test data**: "having chosen a final model, these data are used to estimate an 
unbiased assessment of the model’s performance, which we refer to as the generalization 
error."


## 2. Modelling in R
There are different ways to create a formulas depending on the engine used.
In order to test the model, we should not use the test data, instead, training
data should be splitied using resampling methods, 

## 3. Resampling methods 
"Provide an alternative approach by allowing us to repeatedly fit a model of 
interest to parts of the training data and test its performance on other parts. 
The two most commonly used resampling methods include `k-fold cross validation` 
and `bootstrapping`. "

::: {.panel-tabset}

### K-fold cross validation
Principal idea of k-fold where the training data is divided into training samples
and one testing sample, so you can test within the fold created. This procedure
is repeated k times. In practices, k = 5 or k = 10 is common.

"Although using k ≥ 10 helps to minimize the variability in the estimated performance, 
k-fold CV still tends to have higher variability than bootstrapping (discussed next). 
Kim (2009) showed that repeating k-fold CV can help to increase the precision of 
the estimated generalization error. Consequently, for smaller data sets (say
n<10,000, 10-fold CV repeated 5 or 10 times will improve the accuracy of your 
estimated performance and also provide an estimate of its variability."
```{r}
#| include: true
#| eval: true
rsample::vfold_cv(modeldata::ames, v = 10)
```

### Bootstrapping
Random samples of the data with _replacement_
"Since observations are replicated in bootstrapping, there tends to be less 
variability in the error measure compared with k-fold CV (Efron 1983). However, 
this can also increase the bias of your error estimate. This can be problematic 
with smaller data sets; however, for most average-to-large data sets (say n≥1,000) 
this concern is often negligible."
```{r}
rsample::bootstraps(modeldata::ames, times = 10)
```

::: 

Following image shows distribution for each approach, each graphs was generated 
from the book.
![](../../image/kfolds_vs_bootstrap.png)


## 4. Bias variance trade-off
[Bias variance trade-off](https://bradleyboehmke.github.io/HOML/process.html#bias-var)

## 5. Model evaluation
[Model evaluation](https://bradleyboehmke.github.io/HOML/process.html#model-eval)

# Feature & Target Engineering
I will be using `recipes` packages from tidymodels framework.

## 1. Target Engineering
Some models, for example, parametrics ones. Assumes that their response variable
and the error are normally distributed. Therefore, it is important to review
distribution before start modelling, this might improve the prediction.

One way to correct not normally distribution is with the `log` or `BoxCox` function. 
*"However, we should think of the preprocessing as creating a blueprint to be 
re-applied strategically. For this, you can use the `recipe package` or something 
similar (e.g., caret::preProcess()). This will not return the actual log 
transformed values but, rather, a blueprint to be applied later."*

```{r}
#| eval: false
# Log transformation applied to all outcomes
ames_recipe <- recipe(Sale_Price ~ ., data = ames_train) %>%
  step_log(all_outcomes()) # OR
  # step_BoxCox(all_outcomes())
```

In case the response variable has negatives, the previous approach might conduct
to NAs values, then, `step_YeoJohnson()` can be applied.

## 2. Dealing with missingness
I strongly recommend to use `naniar` package to check missings values in the df.
**naniar::vis_miss()**

![](../../image/vis_miss.PNG)

Some missing values might be an error caused by the construction of the data, so, this 
requires to analyse.
In case, Data is well built, imputation values can be used. Please check the 
following:

![](../../image/missing_imputations.PNG)