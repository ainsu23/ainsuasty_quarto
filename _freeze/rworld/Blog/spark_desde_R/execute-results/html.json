{
  "hash": "0e69d56a61840729f15cccc20f309a90",
  "result": {
    "markdown": "---\ntitle: \"Uso de Spark desde R\"\ndescription: \"Big data con Spark y R\" \nlang: es\ncategories: [\"Spark\", \"R6Class\", \"Text mining\"]\nexecute: \n  freeze: auto\ncomments: \n  utterances: \n    repo: ainsu23/ainsuasty_quarto\n---\n\n\nEl uso de `Spark` en DataScience es ahora más común por sus grandes beneficios. \nAlgunos son: almacenamiento distribuido, uso de queries como si se estuviera \nescribiendo en SQL, desarrollo de modelos de machine learning, entre muchos otros.\n\nRstudio (Posit) ha desarrollado el paquete `sparklyr`, el cual me parece que es\nmuy completo. Súper recomendado!!\n\nLos ejercicios de este blog provienen del curso de Udemy: **\"Taming Big Data with \nApache Spark and Python - Hands On!\"** sin embargo se resolvieron con Sparklyr y \nla conexión se realiza de forma local.\n\n## Cargue librerías\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(R6)\nlibrary(tidyverse)\nlibrary(sparklyr)\n```\n:::\n\n\n\n## Clase R6 spark_conexion.\nLa clase spark_conexion mantiene toda información y métodos relevantes de la \nconexión a Spark. Permitiendo reutilizarlas en todo el código.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_conexion <- R6::R6Class(\n  classname = \"conexion\",\n  public = list(\n    initialize = function() {\n      return(invisible(self))\n    },\n    #' @description\n    #' Crea nuevo objeto de conexión\n    #' @details\n    #' Esta función requiere unas variables de ambiente cargadas para poder\n    #' functionar.\n    connect = function() {\n     self$conn_sp <- sparklyr::spark_connect(master = \"local\")\n      return(invisible(self))\n    },\n    #' @field conn_spark conexion a spack\n    conn_sp  = NULL,\n    #' @descripcion Método para dplyr::copy_to con conexion a spark\n    #' @param tabla_df Data frame\n    #' @param tbl_name Nombre de la tabla en spark\n    copy_to_sp = function(tabla_df, tbl_name) {\n      copy_to(self$conn_sp, tabla_df, tbl_name) \n    },\n    #' @descripcion Método para leer tabla de spark\n    #' @param tbl_name Nombre de la tabla en spark\n    tbl_sp = function(tbl_name) spark_read_table(self$conn_sp, tbl_name)\n  )\n)\n\nconn <- spark_conexion$new()$connect()\n```\n:::\n\n\n## Operaciones básicas con `dplyr`\nArchivo extraído de `grouplens.org`.\n\nVentajas de usar spark desde R:\n\n -    Se pueden usar los *verbos* de dplyr\n -    Lazy evaluation: Ver que en la parte superior del \n  resultado aparece: *# Source: spark<?> [?? x 2])*\n\n::: {.panel-tabset}\n\n## `group_by y count`\n\n::: {.cell}\n\n```{.r .cell-code}\nsparklyr::spark_read_text(\n  conn$conn_sp, \n  name = \"movieLens\",\n  \"../../data/u.data\"\n) %>% \n  separate(line, c(\"user id\",\"item id\",\"rating\",\"timestamp\"), sep = \"\\t\") %>% \n  dplyr::group_by(rating) %>% \n  dplyr::count()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Source: spark<?> [?? x 2]\n  rating     n\n  <chr>  <dbl>\n1 3      27145\n2 1       6110\n3 2      11370\n4 4      34174\n5 5      21201\n```\n:::\n:::\n\n\n## `summarise, arrange`\nTambién permite usar `summarise, arrange` y operaciones dentro de summarise cómo\n`round, mean`.\n\n::: {.cell}\n\n```{.r .cell-code}\nsp_fake_friends <- sparklyr::spark_read_csv(\n  conn$conn_sp,\n  name = \"fakefriends\",\n  \"../../data/fakefriends.csv\",\n  header = FALSE,\n  columns = c(\"id\", \"name\", \"age\", \"num_friends\")\n) %>% \n  dplyr::group_by(age) %>% \n  dplyr::summarise(num_friends = round(mean(num_friends), 1)) %>% \n  dplyr::arrange(age)\nsp_fake_friends\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Source:     spark<?> [?? x 2]\n# Ordered by: age\n   age   num_friends\n   <chr>       <dbl>\n 1 18           343.\n 2 19           213.\n 3 20           165 \n 4 21           351.\n 5 22           206.\n 6 23           246.\n 7 24           234.\n 8 25           198.\n 9 26           242.\n10 27           228.\n# … with more rows\n```\n:::\n:::\n\n\n## filter min \nLa función dplyr::filter puede entrar en conflicto con la funcion sparklyr::filter\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsparklyr::spark_read_csv(\n  conn$conn_sp,\n  name = \"fakefriends\",\n  \"../../data/fakefriends.csv\",\n  header = FALSE,\n  columns = c(\"id\", \"name\", \"age\", \"num_friends\")\n) %>% \n  dplyr::filter(age == min(age))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Source: spark<?> [?? x 4]\n  id    name    age   num_friends\n  <chr> <chr>   <chr> <chr>      \n1 106   Beverly 18    499        \n2 115   Dukat   18    397        \n3 341   Data    18    326        \n4 377   Beverly 18    418        \n5 404   Kasidy  18    24         \n6 439   Data    18    417        \n7 444   Keiko   18    472        \n8 494   Kasidy  18    194        \n```\n:::\n:::\n\n\n:::\n\n\n## Operaciones con texto\nLas operaciones con texto también pueden ser usadas mediante verbos o secuencia\n`tidyverse`. \n\n -    ft_tokenizer: Esta función permite almacenar las palabras de la fila en una \n  lista.     \n -    ft_stop_words_remover: Se eliminan las palabras conexión tales como: `a, en,\n  entre, o, aquí, aún, con, de, e, y, hay, ...`\n   \n\n::: {.cell}\n\n```{.r .cell-code}\nsparklyr::spark_read_text(\n  conn$conn_sp,\n  path = \"../../data/Book\"\n) %>% \n  ft_tokenizer(\n    input_col = \"line\",\n    output_col = \"word_list\"\n      \n  ) %>% \n  ft_stop_words_remover(\n    input_col = \"word_list\",\n    output_col = \"wo_stop_words\"\n  ) %>% \n  dplyr::mutate(palabra = explode(wo_stop_words)) %>% \n  dplyr::filter(palabra != \"\") %>% \n  dplyr::group_by(palabra) %>% \n  dplyr::count() %>% \n  dplyr::filter(palabra != \"�\") %>% \n  dplyr::arrange(desc(n)) %>% \n  head(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Source:     spark<?> [?? x 2]\n# Ordered by: desc(n)\n   palabra      n\n   <chr>    <dbl>\n 1 business   290\n 2 time       168\n 3 need       167\n 4 new        150\n 5 product    128\n 6 people     127\n 7 get        122\n 8 work       120\n 9 may        107\n10 want       107\n```\n:::\n:::\n\n\n\n## Machine Learning \n\nEn mi opinión los beneficios que encontré de aplicar ML con sparklyr son:\n\n -    Pipelines: Conjunto de pasos que se desean aplicar al modelo en \n  construcción, es decir, las operaciones a la base, la formula del modelo, \n  seleccion el algoritmo a desarrollar (regresión lineal, árbol de decisión).\n\n - Algoritmos: Sparklyr usa la librería de ML de Spark, por ende, cuenta con \n  una gran variedad de algoritmos para ser usados.\n \n -    Transformaciones: ft_dplyr_transformer permite aplicar operaciones con dplyr y \n aplicarlo en el pipeline creado.\n\n### Linear Regression\n\n::: {.panel-tabset}\n\n## data\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nsdf_regresion <- sparklyr::spark_read_text(\n  conn$conn_sp,\n  path = \"../../data/regression.txt\"\n) %>% \n  separate(line, c(\"x\", \"y\"), \",\") %>% \n  mutate(across(where(is.character), as.numeric))\n\nsdf_regresion\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Source: spark<?> [?? x 2]\n       x     y\n   <dbl> <dbl>\n 1 -1.74  1.66\n 2  1.24 -1.18\n 3  0.29 -0.4 \n 4 -0.13  0.09\n 5 -0.39  0.38\n 6 -1.79  1.73\n 7  0.71 -0.77\n 8  1.39 -1.48\n 9  1.15 -1.43\n10  0.13 -0.07\n# … with more rows\n```\n:::\n:::\n\n\n## Modelo \n\n::: {.cell}\n\n```{.r .cell-code}\nregresion_pipeline <- sparklyr::ml_pipeline(conn$conn_sp) %>%\n  sparklyr::ft_r_formula(y ~ x) %>%\n  sparklyr::ml_linear_regression()\n\npartitioned_regresion <- sparklyr::sdf_random_split(\n  sdf_regresion,\n  training = 0.7,\n  testing = 0.3\n)\n\nfitted_pipeline <- sparklyr::ml_fit(\n  regresion_pipeline,\n  partitioned_regresion$training\n)\n\npredictions <- sparklyr::ml_transform(\n  fitted_pipeline,\n  partitioned_regresion$testing\n)\n\npredictions\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Source: spark<?> [?? x 5]\n       x     y features  label prediction\n   <dbl> <dbl> <list>    <dbl>      <dbl>\n 1 -3.54  3.44 <dbl [1]>  3.44       3.53\n 2 -2.45  2.44 <dbl [1]>  2.44       2.44\n 3 -2.22  2.15 <dbl [1]>  2.15       2.21\n 4 -2.17  2.19 <dbl [1]>  2.19       2.16\n 5 -2     2.02 <dbl [1]>  2.02       1.99\n 6 -1.94  1.94 <dbl [1]>  1.94       1.93\n 7 -1.91  1.83 <dbl [1]>  1.83       1.90\n 8 -1.91  1.86 <dbl [1]>  1.86       1.90\n 9 -1.91  1.95 <dbl [1]>  1.95       1.90\n10 -1.83  1.68 <dbl [1]>  1.68       1.82\n# … with more rows\n```\n:::\n:::\n\n\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}